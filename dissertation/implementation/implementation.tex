\documentclass[oneside, class=book, 12pt, crop=false]{standalone}

\usepackage{../dissertationstyle}

\bibliography{../personal}

\begin{document}

\ifstandalone
  \graphicspath{ {./images/} }
  \setcounter{chapter}{2}
  \chapter{Implementation}
\fi
\resetfigpath{implementation}

In this chapter, we discuss the software that was produced during this project. We begin by discussing the structure of the project at a high-level, and then cover the high-level structure of the source code repository. Then, we detail and justify the algorithms used for the metric calculators, similarity scorers, and transformations.

\section{Project Structure}

The project consists of two major components: data synthesis and the classifier. The data synthesis module is relatively simple, and was only of use for early testing stages of the project where real-world data was not available. Because of this, we only briefly discuss the data synthesis component.

A diagram outlining the structure of the data synthesis component is given in Figure \ref{fig:datasynthesisflow}. A MusicXML score is passed into the parser, which converts the score into a more useful form for audio generation. This parsed score, along with a pianist profile fed with some parameters, are passed into an audio generation component, which applies slight modifications to the score based on the pianist profile, and finally synthesises the audio as a WAV file.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
  squarednode/.style={rectangle, draw=black!100, fill=black!10, very thick, minimum size=6.5mm},
  datanode/.style={rectangle, draw=black!100, fill=black!0, very thick, minimum size=6.5mm},
  ]
  %Nodes
  \node[datanode] (mxl) {MusicXML Score};
  \node[squarednode] (mxlparser) [right= of mxl] {MusicXML Parser};
  \node[squarednode] (profile) [below= 2.5cm of mxlparser] {Pianist Profile};
  \node[datanode] (tempoenvelope) [below left=of profile] {Tempo Envelope};
  \node[datanode] (amplitudedist) [below= 2 of profile] {Amplitude Distribution};
  \node[datanode] (accuracy) [below right=  of profile] {Timing Accuracy};
  \node[squarednode] (audiogenerator) [below right=of mxlparser] {Audio Generator};
  \node[datanode] (audio) [right= of audiogenerator] {Audio};

  \draw[->] (mxl.east) -- (mxlparser.west);
  \draw[->] (tempoenvelope.east) -| ([xshift=-5pt]profile.south);
  \draw[->] (amplitudedist.north) -- (profile.south);
  \draw[->] (accuracy.west) -| ([xshift=5pt]profile.south);
  \draw[->] (mxlparser.south) |- ([yshift=3pt]audiogenerator.west);
  \draw[->] (profile.north) |- ([yshift=-3pt]audiogenerator.west);
  \draw[->] (audiogenerator.east) -- (audio.west);
  \end{tikzpicture}
  \caption{Flowchart of the data synthesis component}
  \label{fig:datasynthesisflow}
    
\end{figure}


A diagram outlining the structure of the classifier is given in Figure \ref{fig:classifierflow}. Known recordings are passed into the metric calculators, and their metrics stored. To find the performer of an unknown recording, we calculate its metrics, and compare these to the metrics of the known performances using the similarity scorer. We take the most similar performance, and that gives us our performer.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
  squarednode/.style={rectangle, draw=black!100, fill=black!10, very thick, minimum size=6.5mm},
  datanode/.style={rectangle, draw=black!100, fill=black!0, very thick, minimum size=6.5mm},
  ]
  %Nodes
  \foreach \x in {2.0, 2.1, 2.2}\node[datanode] at (\x, \x) (knownrecordings) {Known recordings};
  \node[squarednode]      (metriccalculators1)       [right= of knownrecordings] {Metric calculators};
  \node[database, label=left:Stored Metrics, database radius=0.75cm, database segment height=0.375cm]         (storedmetrics)            [below=of metriccalculators1] {};
  \node[squarednode]      (metriccalculators2) [below= 2.5cm of storedmetrics] {Metric calculators};
  \node[datanode]      (unknownrecording)  [left= of metriccalculators2] {Unknown recording};
  \node[squarednode]      (similaritycalculator) [below right= 1cm and 2cm of storedmetrics] {Similarity scorer};
  \node[datanode]       (performer) [right= of similaritycalculator] {Performer};

  %Lines
  \draw[->] (knownrecordings.east) -- (metriccalculators1.west);
  \draw[->] (metriccalculators1.south) -- (storedmetrics.north);
  \draw[->] (unknownrecording.east)  -- (metriccalculators2.west);
  \draw[->] (storedmetrics.south) |- ([yshift=3pt]similaritycalculator.west);
  \draw[->] (metriccalculators2.north) |- ([yshift=-3pt]similaritycalculator.west);
  \draw[->] (similaritycalculator.east) -- (performer.west);

  \end{tikzpicture}

\caption{Flowchart of the core of the classifier}
\label{fig:classifierflow}
\end{figure}

\section{Repository Overview}

A high-level overview of the repository is given in Figure \ref{fig:repositoryoverview}. A high-level distinction is made between source code in the \texttt{src/} directory and data/resources in the \texttt{res/} directory. Within each of these directories, files are further grouped by their purpose and what component of the overall system they belong to.

\begin{figure}[h]
\begin{verbatim}
|-- res/
|   |-- data/
|   |   contains all of the piano recordings
|   |-- irs/
|   |   contains samples of impulse responses for reverb
|   |-- noise/
|   |   contains samples of background noise
|   |-- scores/
|   |   contains MusicXML scores
|   `-- soundfonts/
|       contains soundfonts for data synthesis
`-- src/
    |-- classifier/
    |   |   contains a variety of utility files
    |   |-- metrics/
    |   |   contains metric calculators and similarity scorers
    |   `-- transformations/
    |       contains transformation implementations
    `-- data_synthesis/
        contains all of the data synthesis implementation
\end{verbatim}
\caption{Repository overview}
\label{fig:repositoryoverview}
\end{figure}

This structure allows for modularity: it is very easy to create a new metric or transformation, you simply have to extend \texttt{src/metrics/metric.py} or \texttt{src/transformations/transformation.py} appropiately.

All of the code was written by the dissertation author. All files in the \texttt{res/data/} directory were created by the dissertation author, and all other files in the \texttt{res/} directory were found online.

Files in the \texttt{irs/} directory were taken from the University of York's \href{https://www.openairlib.net}{OpenAir} project under the Creative Commons 4.0 License\footnote{\url{https://creativecommons.org/licenses/by/4.0/}}.

All other files were taken using the Creative Commons 1.0 license.

\section{Metric Calculators}

In this section we detail and justify the specific algorithms used for each of the metric calculators.

\subsection{Tempo Variation Over Time}

The computation of this metric builds closely on top of the beat tracking algorithm described in Section \ref{sec:beattracking}. In fact, all we do is run our beat tracking algorithm on our audio and take the first-order difference between our beat times, and then apply smoothing by taking the moving average to reduce the impact of micro-variations in note timings (which we instead capture with the note offsets metric). 

There were a few other choices for constructing this metric, for example instead of taking the first-order difference of our beat times, we could simply use the beat times themselves. To see the issues with this approach, suppose we have two performances that have very similar tempos in the latter half, but are different in the first half. We would like to consider these performances reasonably similar since the tempos are similar in the latter half, but by taking the absolute beat times we are unable to find any similarity since the differing tempos in the first half will offset the beat times in the latter half. Taking the first-order difference avoids this issue. Another option might be taking higher-order differences, which seems like it might have some merits. For example, a second-order difference would allow us to easily find where a piece is speeding up/slowing down as opposed to just fast/slow, but by doing this we lose the information on whether the piece actually is being played fast/slow, which isn't ideal.

Furthermore, the smoothing we perform is very important. Being able to avoid these micro-variations is incredibly useful, since this metric really intends to capture an overall view of how tempo changes over the performance of the piece, and not a microscopic view of timings within each bar, for example.

We are also able to tune how much of this smoothing we get on our beat time differences by changing the window size of our moving average. For our case, we choose a window size of 4 beats, which corresponds to the length of a bar (the next level up in the temporal structure of music) in most Western European music, which is what our dataset consists of.

\subsubsection{Similarity Scorer}

To calculate the similarity between two tempo metrics we use a technique which we will frequently see in the similarity scorers for other metrics: calculating the mean squared error. We can consider our tempo metric as being a function of beat number, returning us the time difference between the last beat. Then, to calculate the similarity between two metrics, we just take the mean squared error between these two functions (truncating the longer function if necessary), giving us a number we call $\varepsilon$.

This is not yet useful as a similarity metric, however, since it can grow unboundedly large, so we return the error as $e^{-\varepsilon}$. The intuition behind the choice of this transformation is that if our two performances are the same, we expect $\varepsilon$ to be equal to 0, and thus $e^{-\varepsilon}$ will be 1. As $\varepsilon$ grows, $e^{-\varepsilon}$ will approach 0, which gives us a useful similarity metric.

\subsection{Dynamics over time}










\ifstandalone
  \printbibliography
\fi
    
\end{document}
