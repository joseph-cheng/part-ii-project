\documentclass[oneside, class=book, crop=false, 12pt]{standalone}

\usepackage{../dissertationstyle}

\bibliography{../personal}

\begin{document}


\ifstandalone
  \setcounter{chapter}{0}
  \chapter{Introduction}
\fi
\resetfigpath{introduction}

This chapter explains the motivation for this project, provides an overview of the project, and discusses previous related work in this area.

\section{Motivation}

Individuals have many characteristics that can be used to identify them, ranging from immutable characteristics of a person like biometrics, to qualities of an action they perform, such as their handwriting. This project explores the use of another characteristic that can be used to identify a person: how they play a particular piece on the piano. 

A motivating use-case for this is seen as follows: suppose 10 people play a particular piece of music on the piano and we have audio recordings for each of these performances. If one of these performers plays the piece again, how can we determine which performer this was, from just the audio recordings? Such a problem requires an approach that utilises techniques from both digital signal processing (DSP) and music information retrival (MIR).

In order to pull useful information out of an audio recording related to a pianist's performance, we need to process the signal in some meaningful way to gather musical information from it. For example, the discrete-time Fourier transform (DTFT) is used to transform a discrete-time audio signal into its frequency data. This technique is invaluable and has a myriad of uses, for example being able to get data about what notes are being played at a given time, or to attempt to quantify the timbre of a particular sound.

\section{Project Overview}

The system created in this project works as follows: given performances of the same piece by different pianists, we compute a number of metrics for each performance and store them. To try and determine the performer of a performance with an unknown performer, we compute the same metrics on this performance, and then compute similarities between the metrics of this performance and the metrics of the other performances, and choose the performer for which we get the highest aggregate similarity.

In total, the project implements five metrics:

\begin{itemize}
  \item
    Tempo variation over time

  \item
    Dynamics over time

  \item
    Chroma vector extraction

  \item
    Note offsets

  \item
    Timbre extraction
\end{itemize}

An explanation of how we compute each of these metrics, and what form they take is given in Chapter \ref{chapter:implementation}

The project explores which of these metrics are most useful, and quantifies the usefulness of each metric, and looks at how the metrics perform under different conditions (e.g. addition of white noise)


\section{Related work}

There is some relevant work already done that is adjacent to this topic, for example there is a paper that investigates how individual a pianist's performance is, exploring many different metrics \cite{bernays14}, but this paper uses a symbolic representation of the performances with MIDI, as opposed to audio of the performance itself.

There is no similar work that operates on audio signals as far as I am aware, but there is some work that uses raw audio signals of a sung password using the timbre of the voice as biometric authentication \cite{prakash16}, which whilst not the same as the work in this project, is relevant.

Furthermore, there is plenty of research on beat tracking, in particular we use the techniques by Ellis \cite{ellis07}, which is a vital component of our project.

\ifstandalone
  \printbibliography[heading=subbibliography]
\fi
    
\end{document}
