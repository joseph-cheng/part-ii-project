\documentclass[oneside, class=book, 12pt, crop=false]{standalone}

\usepackage{../dissertationstyle}

\bibliography{../personal}

\begin{document}

\ifstandalone
  \setcounter{chapter}{3}
  \chapter{Evaluation}
\fi
\resetfigpath{evaluation}
% write about:
% individual metric capabilities, check in implementation for combos
% peak metric success
% how each metric and peak metric success were affected by variations
% plot graph of noise vs time

In this chapter, we evaluate the success of the project, with reference to the requirements given in Table \ref{table:requirements}. We also explore the absolute performance of the system, as well as comparative success, as well as theorising about underlying causes of changes in performance. 

It should be noted that graphs/data presented in this chapter are unusually not shown with errors. This is because the system is entirely deterministic, and thus there is no way to meaningfully generate errors.

\section{Requirements Analysis Revisited}

All of the core requirements were successfully completed:

\begin{itemize}
  \item
    \textbf{Calculators for each of the 5 proposed metrics}

    As explained in Chapter \ref{chapter:implementation}, each of the metrics were implemented in such a way that they successfully capture information outlined in Chapter \ref{chapter:introduction}. We will see evidence of this success later in the chapter.

  \item
    \textbf{Similarity scorers for each of the 5 proposed metrics}

    Again, we have seen that we were able to implement similarity scorers for each of the metrics, and the evidence we see later in this chapter tells us that these capture sensible notions of similarity.

  \item
    \textbf{Additional of optional variation to input data}

    As seen in Section \ref{sec:transformations}, we are able to add variation to input data such that we are able to somewhat realistically mimic sonic effects that could be perceived in other recordings.

  \item
    \textbf{Code to automatically perform evaluation based on a plan}

    As we will see in Section \ref{sec:evaluation framework}, we were able to implement a framework for evaluating the system given a number of parameters, allowing for easy gathering of data.

  \item
    \textbf{Data synthesis engine to generate test piano data}

    As seen in Section \ref{sec:datasynthesis}, we implemented an engine that was able to parse scores and synthesise audio with added variation that aided in development of the core system.

  \item
    \textbf{Data synthesis engine to generate test data from other instruments}

    This was an extension requirement, and was unable to be completed. Generating good test data from other instruments would require more than a surface level understanding of the instrument, and I did not have enough time to both do this research and implement a data synthesis engine based on my findings.

  \item
    \textbf{Gathering of real-world piano performances}

    As outlined in Section \ref{sec:datacollection}, we successfully gathered a total of 32  real-world piano performances, using a range of piano pieces that cover different techniques (and thus sound different).
\end{itemize}

\section{Evaluation of Metrics}

In this section, we discuss how each metric and metric combination were evaluated, as well as the framework developed to aid implementation.

To evaluate our system, we only really care about one statistic: does the system guess the correct performer given one distinguished performance and a set of performances where one is by the same performer as the distinguished performance. Furthermore, since our dataset is relatively small, it is important to try and use this dataset in many ways to gather enough evaluation information. For these reasons, in order to evaluate the system for a set of performances $P$ containing two performances of the same piece for each pianist, we do the following:

\begin{itemize}
  \item
    Initialise a counter $c$ to 0.

  \item
    Choose each performance from $P$ as the `distinguished' performance $p$ in turn.

    \begin{itemize}
        

      \item
        Calculate metrics for all of the performances in $P$.

      \item
        Calculate the mean similarity over each of the metrics between the distinguished performance $p$ and  $p' \in P \setminus \{p\}$.

      \item
        Take the system's guess $g$ as the performance with the highest mean similarity.
      \item
        If $p$ has the same performer as $g$, increment $c$.
    \end{itemize}

  \item
    The success for $P$ is then the $c$ divided by the total number of guesses our system made, so $\frac{c}{|P|}$.
\end{itemize}

We can then extend this if we have multiple collections $P$ by simply taking the mean success, weighted by the size of each collection.

This method allows us to get a lot of evaluation data from just a few performances, for a set of performances $P$ we get a granularity of $\frac{1}{|P|}$. In our case, each of our sets of performances have 8 performances each, and we have 4 different pieces, which gives us a granularity of $\frac{1}{32}$.

It should be noted here that it may seem like we are inflating the number of trials (which is important to get right for hypothesis testing), since if we have two performances $p_1$ and $p_2$ by the same performer, we have one instance where we choose $p_1$ to be our distinguished performance, and another where we choose $p_2$, and by symmetry of our similarity scorers it may seem like this is a bad idea. However, there could be a third performance $p_3$ that $p_2$ has the highest aggregate similarity with, but does not have the highest aggregate similarity with $p_1$, so we are not inflating the number of trials.


We can go even further, and find what success we need for a result to be statistically significantly different from random. For a set of performances $P$, if our system guessed at random, we would have a $\frac{1}{|P|-1}$ probability of guessing correctly. Now, suppose we have $n$ sets of performances $P_1, \ldots, P_n$. To match our dataset, we will assume that the $P_i$ all have the same size. This means that we can model the number of successful guesses of our system as a random variable modelled by a binomial distribution $X \sim B(n|P_i|, \frac{1}{|P_i|-1})$. At a significance level of $p$, we then just need to find the least $m$ such that $P(X \geq m) \leq p$.

For our dataset, we have $n=4$, $|P_i| = 8$, and we set $p$ at 0.05. We then find that $P(X \geq 7) = 0.076$ (2 s.f.) and $P(X \geq 8) = 0.031$ (2 s.f.), so our critical value is 8. This means we only need to get 8 successful trials, or a success of 0.25, for our system to be statistically significantly better than random.

\subsection{Evaluation Framework}\label{sec:evaluation framework}

To speed up evaluation, it is important to easily be able to evaluate the entire system (as described earlier) on any combination of metrics, and with any number of transformations. Furthermore, the evaluation needs to be reasonably fast since we need to evaluate the system over a very large parameter space.

For this reason, we implement an \texttt{evaluate\_metrics} function in \texttt{src/classifier/evaluation.py} that, given a list of metric calculators and a list of transformations, evaluates the entire system and returns its success. Furthermore, a number of caching mechanisms are implemented that are vital to the success of evaluation: the time it would take to evaluate the system would be nearly infeasible without them. Calculating the metrics and similarities involves rather expensive operation and algorithms, so metrics are cached. Additionally, we cache onset functions, since they are relatively expensive to compute, and end up underpinning a lot of metric calculators/similarity scorers.

\section{Metric Performance}

In this section we look at the performance of each metric and particular metric combinations, as well as the effects of transformations on the results.

\subsection{Individual Metrics}

A table of the successes for each metric is given in Table \ref{table:metric results}. 

\begin{table}[h]
    \centering
    \begin{tabular}{cc}
        \textbf{Metric}&\textbf{Success} (2 s.f.) \\
        \midrule[0.15em]
        Tempo variation over time&0.31 \\
        Dynamics over time&0.56 \\
         Chroma vector extraction&0.50 \\
         Note offsets&0.66 \\
         Timbre extraction&0.25\\
    \end{tabular}
    \caption{Individual metric results}
    \label{table:metric results}
\end{table}

Whilst the results for individual metrics are not stellar, they are all statistically significantly different from random (at $p=0.05$) which shows that, to some extent, each of the metrics were devised and implemented sensibly. Three metrics, however, performed significantly better than the other two: dynamics over time, chroma vector extraction, and note offsets. The metric that performed the worst was the timbre extraction metric, which was theorised at the start to not have the most potential. It is possible that the timbre extraction might not even be significantly better than random on its own merit: as explained in Section \ref{sec:timbre metric}, our estimation of normalising the MFCCs on pitch is only crude, and so the performance of this metric might instead be proxying the performance of the chroma metric.

%TODO: remember that poor performance of the tempo metric might be bad scaling
% no clue what this means ^

\subsection{Metric Combinations}

Whilst individual metrics were not able to perform extremely well, much better performance could be attained by combining multiple metrics together. For example, as theorised in Section \ref{sec:chroma similarity}, the combination of both the chroma and dynamics metrics were able to achieve a success of 0.59 (2 s.f.), due to there being some pieces/performers that the chroma metric was good at differentiating, and some distinct pieces/performers that the dynamics metric was good at differentiating.

By just combining two metrics, the best success attained was the combination of the dynamics and offsets metrics (the two highest performing individual metrics), achieving a success of 0.75 (2 s.f.).

Even better results could be attained by combining 3 or 4 metrics. The two best metric combinations were found by combining the chroma, dynamics, and offsets metrics, and by combining the chroma, dynamics, offsets, and timbre metrics, both achieving a success of 0.78 (2 s.f.). Since the only difference between these two sets of metrics is the addition of the timbre metric, this provides even more evidence that the timbre metric is not that useful. In fact, combining any other metric $M$ with the timbre metric did not result in a different success to having just $M$ by itself.

Interestingly, combining all 5 metrics only resulted in a success of 0.69 (2 s.f.), which implies that the tempo metric is somehow antagonistic with some other metrics in our system.

By investigating further, we find that only the chroma metric received an increase in success when combined with the tempo metric (increasing from 0.50 to 0.59 (2 s.f.)), whereas all other metrics (ignoring timbre) took a decrease in success of 28\% and 10\% for dynamics and note offsets respectively. So, even though there is some affinity between the chroma and tempo metrics, this is not enough to outweigh the antagonism between the tempo metric and the dynamics and note offsets metrics, resulting in a decrease in performance when we use all 5 metrics.

\subsection{Performance under Transformations}

The performance of the metrics under noise and reverb transformations is somewhat interesting. In Table \ref{table:transformation results}, we give the success for each individual metric under each combination of transformations, including no transformations (we provide this just for completeness, it is just the same data we store in Table \ref{table:metric results}).

The impulse response used for generating reverb for these results is the impulse response of a recording studio.

\begin{table}[h]
    \centering
    \begin{tabular}{c|cc}
        \textbf{Transformation}&\textbf{Metric}&\textbf{Success} (2 s.f.) \\
        \midrule[0.15em]
        \multirow{5}{*}{None} &Tempo variation over time&0.31 \\ \cline{2-3}
                              &Dynamics over time&0.56 \\ \cline{2-3}
                              &Chroma vector extraction&0.50 \\ \cline{2-3}
                              &Note offsets&0.66 \\ \cline{2-3}
                              &Timbre extraction&0.25\\ \midrule[0.1em]
        \multirow{5}{*}{Noise} & Tempo variation over time & 0.31 \\ \cline{2-3}
                               &Dynamics over time & 0.47 \\ \cline{2-3}
                               &Chroma vector extraction & 0.53 \\ \cline{2-3}
                               &Note offsets & 0.63 \\ \cline{2-3}
                               &Timbre extraction & 0.25 \\ \midrule[0.1em]
        \multirow{5}{*}{Reverb} & Tempo variation over time & 0.38 \\ \cline{2-3}
                                &Dynamics over time & 0.53 \\ \cline{2-3}
                                &Chroma vector extraction & 0.38 \\ \cline{2-3}
                                &Note offsets & 0.69 \\ \cline{2-3}
                                &Timbre extraction & 0.25 \\ \midrule[0.1em]

        \multirow{5}{*}{Noise + Reverb} & Tempo variation over time & 0.41 \\ \cline{2-3}
                                &Dynamics over time & 0.50 \\ \cline{2-3}
                                &Chroma vector extraction & 0.41 \\ \cline{2-3}
                                &Note offsets &  0.63\\ \cline{2-3}
                                &Timbre extraction & 0.25 \\ 
        
    \end{tabular}
    \caption{Individual metric results under various transformations}
    \label{table:transformation results}
\end{table}



As we can see, introduction of transformations causes some performances to have a reduction in performance, but bolsters others. The timbre metric remains completely unaffected by any transformation, which again implies that it is not much better than random.

When looking into the performance of metric combinations, we see that the noise transformation actually results in a better peak success: 0.81 (2 s.f.) with the same best metric combinations for no transformations. When adding the reverb transformation, our peak success decreases, only being able to attain a peak success of 0.69 (2 s.f.), which is no better than the success obtained by the note offsets metrics on its own. Interestingly, we can see that the chroma metric took a large performance hit, and none of the best metric combinations include the chroma metric. Finally, applying noise and then reverb allows us to attain a peak success of 0.75 (2 s.f.) with numerous metric combinations, but none including the chroma metric.

\subsection{Digging a Bit Deeper}

In this section, we explore the behaviour of these result further, looking at how the system changes upon varying these transformations, and some underlying causes within the system that cause this behaviour.

\subsubsection{Noise}

The results for noise seem counter-intuitive, since we seem to be gaining performance by adding noise into our system. The graph in Figure \ref{fig:noiseplot} shows how the best and worst metric performance changes as we increase the amplitude of the noise that we add.

\begin{figure}[h]
    \captionsetup{justification=centering}
    \centering
    \includegraphics[scale=0.3]{noiseplot}
    \caption{Best and worst metric performance against noise amplitude (confidence intervals are not given due to the algorithm being deterministic)}
    \label{fig:noiseplot}
\end{figure}

Although the peak performance varies (and sometimes increases), there is a general trend of it decreasing, although it is impressive that the system is still able to perform reasonably well under very amplified noise. The worst performance, however, suffers tremendously, such that the system is sometimes worse than random (which occurs at a success of 0.125), and is even capable of being unable to correctly identify a single performer at certain points (which is statistically significant at $p=0.05$). This indicates that the noise is somehow acting adversarially to our system, since otherwise we would not expect the system to become worse than random.

Furthermore, the peak performance with additional noise is not statistically significantly different from without noise ($p=0.05$), so we cannot recommend that adding noise is a sensible idea to increase the accuracy of the system.

\subsubsection{Reverb}\label{sec:reverb issues}

One potential cause for the decrease in performance of the chroma metric when adding reverb might be due to the properties of the convolution operation (which we use to implement reverb). Since convolution amplifies frequency components that are shared and diminishes those that are not shared, it is reasonable to expect that chroma vectors extracted from a signal augmented with convolutional reverb will not be as accurate as those extracted from the original signal, and thus the similarity scorer may be unable to distinguish between performers as well. This result still occurs with real reverb, created by playing an audio signal in a room, for example Alvin Lucier's \textit{I Am Sitting In A Room}\cite{alvinlucier}  demonstrates that certain frequencies will attenuate/amplify when real reverb is applied.

Figure \ref{fig:convolutionspectrograms} shows excerpts of spectrograms for a performance before and after reverb is added, along with the spectrogram of the impulse response used to generate the reverb. The spectrogram of the impulse response shows that the power is concentrated mostly in the lower frequencies. Subsequently, power of the signal in the performance after reverb is also mostly concentrated in the lower frequencies. This means that, as theorised, chroma vectors for signals under the effects of this reverb will \textit{all} become biased towards the amplified notes.




\begin{figure}[h]
  \captionsetup{justification=centering}
\begin{minipage}{.33\textwidth}
  \includegraphics[scale=0.65]{before_reverb}
  \subcaption{}
\end{minipage}%
\begin{minipage}{.33\textwidth}
  \flushleft
  \includegraphics[scale=0.65]{after_reverb}
  \subcaption{}
\end{minipage}%
\begin{minipage}{.33\textwidth}
  \centering
  \includegraphics[scale=0.65]{ir}
  \subcaption{}
\end{minipage}
\caption{Spectrograms of a performance before reverb (a), after reverb (b), and the impulse response used to generate reverb (c)}
  \label{fig:convolutionspectrograms}
\end{figure}

Alternatively, this result could become a boon in certain circumstances. For example, if each of the performers record in different rooms, each with their own acoustic properties, then the frequency attenuation and amplification caused by the reverb effects will be unique to each of the performers, potentially making the chroma metric more effective in these circumstances as it becomes a proxy for measuring the acoustic properties of the room.

In fact, when this was tested, assigning each performer a different impulse response (from a recording studio, a church, a valley, and a sports centre), we found that the success of the chroma metric reached 0.78 (2 s.f.), which is statistically significant compared to having no transformations applied ($p=0.05$).

Unfortunately, due to our construction of reverb, we cannot vary the level of reverb that is applied to a sound signal continuously, as we can with our noise (by increasing the amplitude of the noise sample). However, we can use samples of impulse responses that produce more reverb. This is exactly what we did, and gathered results when using an impulse response gathered both in a church and a sports centre, compared to the impulse response used for the initial results in Table \ref{table:transformation results}. The church IR produces more reverb than the studio IR, and the sports center IR more than the church IR. We present the results in Table \ref{table:reverb results}

\begin{table}[h]
  \centering
  \begin{tabular}{cCCC}
    \textbf{Metric}&\textbf{Success with studio reverb} (2 s.f.)&\textbf{Success with church reverb} (2 s.f.)&\textbf{Success with sports centre reverb} (2 s.f.)\\
        \midrule[0.15em]
        Tempo variation over time & 0.38 & 0.47 &0.34\\
        Dynamics over time&0.53&0.69&0.81\\
        Chroma vector extraction&0.38&0.34&0.53\\
        Note offsets&0.69&0.66&0.53\\
        Timbre extraction&0.25&0.25&0.25\\
  \end{tabular}
  \caption{Results for varying levels of reverb}
  \label{table:reverb results}
\end{table}

Most variations in success are not statistically significant ($p=0.05$). However, the dynamics metric shows large (and statistically significant) jumps in success when reverb is added. Again, like the results for adding noise, this is counter-intuitive since we seem to be able to increase performance by making our data `worse'. However, this particular result comes from an implementation detail in the dynamics similarity calculator: in order to calculate the similarity of two dynamics metrics, we truncate the longer metric to match the length of the other. As a result of doing this, the reverb tail of the shorter metric will remain, whereas the reverb tail of the longer metric gets truncated. Since the reverb tail gets very quiet, we end up comparing the loudness of a quiet reverb tail against the loudness of when someone is actually playing notes. This means that when two audio signals are dissimilar in duration, they are marked as dissimilar by the dynamics metric. Since the same performer tends to play the same piece for the same duration, the dynamics metric is able to get high success when we apply more reverb, since there is a longer reverb tail.

Figure \ref{fig:dynamics reverb tails} shows this, where we can see only one reverb tail with different performers, whereas we see two reverb tails with the same performer.

\begin{figure}[h]

  \captionsetup{justification=centering}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[scale=0.2]{dynamics_reverb_diff}
    \subcaption{}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[scale=0.2]{dynamics_reverb_same}
    \subcaption{}
  \end{minipage}

  \caption{Comparing dynamics metrics between different performers (a) and the same performer (b)}
  \label{fig:dynamics reverb tails}
\end{figure}

One important consideration about use of this system that this analysis reveals is that input data should be edited to ensure that periods where there is no playing (both at the start and end of an audio signal) should be cut out, to avoid this scenario. 


\section{Summary}

In this section, we have seen that all of the core requirements of the project have been met, although missing an extension requirement. We have seen that the system is able to obtain reasonably good performance, far surpassing random. We have seen that combining metrics allows a larger portion of the data to be correctly identified, since the spaces of performers that each metric can identify do not overlap completely. Finally, we have also seen that the system is able to sustain most of its performance when the audio is altered by transforming it in some way, and we have looked at some anomalies that arise in the system when transformations are applied and the reasons behind them.












\ifstandalone
  \printbibliography
\fi
    
\end{document}
